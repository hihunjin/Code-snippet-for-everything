{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bi-tempered-loss.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMiSaFPPgB1e3mPUPVI0Di8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hihunjin/Code-snippet-for-everything/blob/main/Bi_tempered_loss.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ao65HstT6wGn"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSDSH4ng6wkf"
      },
      "source": [
        "def log_t(u, t):\n",
        "    \"\"\"Compute log_t for `u'.\"\"\"\n",
        "    if t==1.0:\n",
        "        return u.log()\n",
        "    else:\n",
        "        return (u.pow(1.0 - t) - 1.0) / (1.0 - t)\n",
        "\n",
        "def exp_t(u, t):\n",
        "    \"\"\"Compute exp_t for `u'.\"\"\"\n",
        "    if t==1:\n",
        "        return u.exp()\n",
        "    else:\n",
        "        return (1.0 + (1.0-t)*u).relu().pow(1.0 / (1.0 - t))\n",
        "\n",
        "def compute_normalization_fixed_point(activations, t, num_iters):\n",
        "\n",
        "    \"\"\"Returns the normalization value for each example (t > 1.0).\n",
        "    Args:\n",
        "      activations: A multi-dimensional tensor with last dimension `num_classes`.\n",
        "      t: Temperature 2 (> 1.0 for tail heaviness).\n",
        "      num_iters: Number of iterations to run the method.\n",
        "    Return: A tensor of same shape as activation with the last dimension being 1.\n",
        "    \"\"\"\n",
        "    mu, _ = torch.max(activations, -1, keepdim=True)\n",
        "    normalized_activations_step_0 = activations - mu\n",
        "\n",
        "    normalized_activations = normalized_activations_step_0\n",
        "\n",
        "    for _ in range(num_iters):\n",
        "        logt_partition = torch.sum(\n",
        "                exp_t(normalized_activations, t), -1, keepdim=True)\n",
        "        normalized_activations = normalized_activations_step_0 * \\\n",
        "                logt_partition.pow(1.0-t)\n",
        "\n",
        "    logt_partition = torch.sum(\n",
        "            exp_t(normalized_activations, t), -1, keepdim=True)\n",
        "    normalization_constants = - log_t(1.0 / logt_partition, t) + mu\n",
        "\n",
        "    return normalization_constants\n",
        "\n",
        "def compute_normalization_binary_search(activations, t, num_iters):\n",
        "\n",
        "    \"\"\"Returns the normalization value for each example (t < 1.0).\n",
        "    Args:\n",
        "      activations: A multi-dimensional tensor with last dimension `num_classes`.\n",
        "      t: Temperature 2 (< 1.0 for finite support).\n",
        "      num_iters: Number of iterations to run the method.\n",
        "    Return: A tensor of same rank as activation with the last dimension being 1.\n",
        "    \"\"\"\n",
        "\n",
        "    mu, _ = torch.max(activations, -1, keepdim=True)\n",
        "    normalized_activations = activations - mu\n",
        "\n",
        "    effective_dim = \\\n",
        "        torch.sum(\n",
        "                (normalized_activations > -1.0 / (1.0-t)).to(torch.int32),\n",
        "            dim=-1, keepdim=True).to(activations.dtype)\n",
        "\n",
        "    shape_partition = activations.shape[:-1] + (1,)\n",
        "    lower = torch.zeros(shape_partition, dtype=activations.dtype, device=activations.device)\n",
        "    upper = -log_t(1.0/effective_dim, t) * torch.ones_like(lower)\n",
        "\n",
        "    for _ in range(num_iters):\n",
        "        logt_partition = (upper + lower)/2.0\n",
        "        sum_probs = torch.sum(\n",
        "                exp_t(normalized_activations - logt_partition, t),\n",
        "                dim=-1, keepdim=True)\n",
        "        update = (sum_probs < 1.0).to(activations.dtype)\n",
        "        lower = torch.reshape(\n",
        "                lower * update + (1.0-update) * logt_partition,\n",
        "                shape_partition)\n",
        "        upper = torch.reshape(\n",
        "                upper * (1.0 - update) + update * logt_partition,\n",
        "                shape_partition)\n",
        "\n",
        "    logt_partition = (upper + lower)/2.0\n",
        "    return logt_partition + mu\n",
        "\n",
        "class ComputeNormalization(torch.autograd.Function):\n",
        "    \"\"\"\n",
        "    Class implementing custom backward pass for compute_normalization. See compute_normalization.\n",
        "    \"\"\"\n",
        "    @staticmethod\n",
        "    def forward(ctx, activations, t, num_iters):\n",
        "        if t < 1.0:\n",
        "            normalization_constants = compute_normalization_binary_search(activations, t, num_iters)\n",
        "        else:\n",
        "            normalization_constants = compute_normalization_fixed_point(activations, t, num_iters)\n",
        "\n",
        "        ctx.save_for_backward(activations, normalization_constants)\n",
        "        ctx.t=t\n",
        "        return normalization_constants\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        activations, normalization_constants = ctx.saved_tensors\n",
        "        t = ctx.t\n",
        "        normalized_activations = activations - normalization_constants \n",
        "        probabilities = exp_t(normalized_activations, t)\n",
        "        escorts = probabilities.pow(t)\n",
        "        escorts = escorts / escorts.sum(dim=-1, keepdim=True)\n",
        "        grad_input = escorts * grad_output\n",
        "        \n",
        "        return grad_input, None, None\n",
        "\n",
        "def compute_normalization(activations, t, num_iters=5):\n",
        "    \"\"\"Returns the normalization value for each example. \n",
        "    Backward pass is implemented.\n",
        "    Args:\n",
        "      activations: A multi-dimensional tensor with last dimension `num_classes`.\n",
        "      t: Temperature 2 (> 1.0 for tail heaviness, < 1.0 for finite support).\n",
        "      num_iters: Number of iterations to run the method.\n",
        "    Return: A tensor of same rank as activation with the last dimension being 1.\n",
        "    \"\"\"\n",
        "    return ComputeNormalization.apply(activations, t, num_iters)\n",
        "\n",
        "def tempered_sigmoid(activations, t, num_iters = 5):\n",
        "    \"\"\"Tempered sigmoid function.\n",
        "    Args:\n",
        "      activations: Activations for the positive class for binary classification.\n",
        "      t: Temperature tensor > 0.0.\n",
        "      num_iters: Number of iterations to run the method.\n",
        "    Returns:\n",
        "      A probabilities tensor.\n",
        "    \"\"\"\n",
        "    internal_activations = torch.stack([activations,\n",
        "        torch.zeros_like(activations)],\n",
        "        dim=-1)\n",
        "    internal_probabilities = tempered_softmax(internal_activations, t, num_iters)\n",
        "    return internal_probabilities[..., 0]\n",
        "\n",
        "\n",
        "def tempered_softmax(activations, t, num_iters=5):\n",
        "    \"\"\"Tempered softmax function.\n",
        "    Args:\n",
        "      activations: A multi-dimensional tensor with last dimension `num_classes`.\n",
        "      t: Temperature > 1.0.\n",
        "      num_iters: Number of iterations to run the method.\n",
        "    Returns:\n",
        "      A probabilities tensor.\n",
        "    \"\"\"\n",
        "    if t == 1.0:\n",
        "        return activations.softmax(dim=-1)\n",
        "\n",
        "    normalization_constants = compute_normalization(activations, t, num_iters)\n",
        "    return exp_t(activations - normalization_constants, t)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gR0CyeBw6yAU"
      },
      "source": [
        "def bi_tempered_logistic_loss(activations,\n",
        "        labels,\n",
        "        t1,\n",
        "        t2,\n",
        "        label_smoothing=0.0,\n",
        "        num_iters=5,\n",
        "        reduction = 'mean'):\n",
        "\n",
        "    \"\"\"Bi-Tempered Logistic Loss.\n",
        "    Args:\n",
        "      activations: A multi-dimensional tensor with last dimension `num_classes`.\n",
        "      labels: A tensor with shape and dtype as activations (onehot), \n",
        "        or a long tensor of one dimension less than activations (pytorch standard)\n",
        "      t1: Temperature 1 (< 1.0 for boundedness).\n",
        "      t2: Temperature 2 (> 1.0 for tail heaviness, < 1.0 for finite support).\n",
        "      label_smoothing: Label smoothing parameter between [0, 1). Default 0.0.\n",
        "      num_iters: Number of iterations to run the method. Default 5.\n",
        "      reduction: ``'none'`` | ``'mean'`` | ``'sum'``. Default ``'mean'``.\n",
        "        ``'none'``: No reduction is applied, return shape is shape of\n",
        "        activations without the last dimension.\n",
        "        ``'mean'``: Loss is averaged over minibatch. Return shape (1,)\n",
        "        ``'sum'``: Loss is summed over minibatch. Return shape (1,)\n",
        "    Returns:\n",
        "      A loss tensor.\n",
        "    \"\"\"\n",
        "\n",
        "    if len(labels.shape)<len(activations.shape): #not one-hot\n",
        "        labels_onehot = torch.zeros_like(activations)\n",
        "        labels_onehot.scatter_(1, labels[..., None], 1)\n",
        "    else:\n",
        "        labels_onehot = labels\n",
        "\n",
        "    if label_smoothing > 0:\n",
        "        num_classes = labels_onehot.shape[-1]\n",
        "        labels_onehot = ( 1 - label_smoothing * num_classes / (num_classes - 1) ) \\\n",
        "                * labels_onehot + \\\n",
        "                label_smoothing / (num_classes - 1)\n",
        "\n",
        "    probabilities = tempered_softmax(activations, t2, num_iters)\n",
        "\n",
        "    loss_values = labels_onehot * log_t(labels_onehot + 1e-10, t1) \\\n",
        "            - labels_onehot * log_t(probabilities, t1) \\\n",
        "            - labels_onehot.pow(2.0 - t1) / (2.0 - t1) \\\n",
        "            + probabilities.pow(2.0 - t1) / (2.0 - t1)\n",
        "    loss_values = loss_values.sum(dim = -1) #sum over classes\n",
        "\n",
        "    if reduction == 'none':\n",
        "        return loss_values\n",
        "    if reduction == 'sum':\n",
        "        return loss_values.sum()\n",
        "    if reduction == 'mean':\n",
        "        return loss_values.mean()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkLvpzTs6zlC"
      },
      "source": [
        "class BiTemperedLogisticLoss(nn.Module): \n",
        "    def __init__(self, t1, t2, smoothing=0.0): \n",
        "        super(BiTemperedLogisticLoss, self).__init__() \n",
        "        self.t1 = t1\n",
        "        self.t2 = t2\n",
        "        self.smoothing = smoothing\n",
        "    def forward(self, logit_label, truth_label):\n",
        "        loss_label = bi_tempered_logistic_loss(\n",
        "            logit_label, truth_label,\n",
        "            t1=self.t1, t2=self.t2,\n",
        "            label_smoothing=self.smoothing,\n",
        "            reduction='none'\n",
        "        )\n",
        "        \n",
        "        loss_label = loss_label.mean()\n",
        "        return loss_label\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CaN2h0u60x_"
      },
      "source": [
        "logit = torch.rand(3,10)\n",
        "label = torch.randint(0,10,(3,))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMs327Q561uM",
        "outputId": "12c38ada-4d56-47fc-d191-375e61a62fe3"
      },
      "source": [
        "BiTemperedLogisticLoss(t1=0.3, t2=1.0)(logit,label)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.6689)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sX5FLB1T62q-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}